{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWo8FaiML9Wh"
      },
      "source": [
        "<div><center><img src=\"https://ml.ucv.ai/logo.png\\\" width=150\"/> </center></div>\n",
        "\n",
        "# 04 Segmentación de imágenes con redes U-Net\n",
        "\n",
        "\n",
        "Basado en:\n",
        "\n",
        "- [Efficient Image Segmentation](https://medium.com/data-science/efficient-image-segmentation-using-pytorch-part-3-3534cf04fb89)\n",
        "- [Image segmentation with a U-Net-like architecture](https://keras.io/examples/vision/oxford_pets_image_segmentation/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvG-a4nAOTqS"
      },
      "source": [
        "# 1. Arquitecturas Encoder-Decoder (Teoría y Fórmulas)\n",
        "\n",
        "Un modelo **encoder-decoder** transforma una entrada $ \\mathbf{x} \\in \\mathbb{R}^n $ en una salida $ \\mathbf{y} \\in \\mathbb{R}^m $ mediante una representación latente $ \\mathbf{z} \\in \\mathbb{R}^d $, donde normalmente $ d < n $.\n",
        "\n",
        "## 1.1 Encoder\n",
        "\n",
        "El **encoder** convierte la entrada $ \\mathbf{x} $ en una representación latente $ \\mathbf{z} $:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = f_{\\text{enc}}(\\mathbf{x}; \\theta_e)\n",
        "$$\n",
        "\n",
        "- $ f_{\\text{enc}} $: red neuronal (CNN, RNN, MLP, etc.)\n",
        "- $ \\theta_e $: parámetros del encoder\n",
        "- $ \\mathbf{z} $: vector latente comprimido\n",
        "\n",
        "Si el encoder tiene múltiples capas:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1(\\mathbf{x})\n",
        "$$\n",
        "\n",
        "En términos de dominio y codominio:\n",
        "\n",
        "$$\n",
        "f_{\\text{enc}}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^d\n",
        "$$\n",
        "\n",
        "- $ \\mathbf{x} \\in \\mathbb{R}^n $: entrada original\n",
        "- $ \\mathbf{z} = f_{\\text{enc}}(\\mathbf{x}) \\in \\mathbb{R}^d $: representación latente\n",
        "- Típicamente $ d < n $ para compresión\n",
        "\n",
        "\n",
        "## 1.2. Decoder\n",
        "\n",
        "El **decoder** reconstruye o genera la salida a partir de $ \\mathbf{z} $:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = f_{\\text{dec}}(\\mathbf{z}; \\theta_d)\n",
        "$$\n",
        "\n",
        "- $ f_{\\text{dec}} $: red decodificadora\n",
        "- $ \\theta_d $: parámetros del decoder\n",
        "- $ \\hat{\\mathbf{y}} $: salida generada o reconstruida\n",
        "\n",
        "Reconstruye o genera la salida desde el espacio latente:\n",
        "\n",
        "$$\n",
        "f_{\\text{dec}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^m\n",
        "$$\n",
        "\n",
        "- $ \\mathbf{z} \\in \\mathbb{R}^d $: vector latente\n",
        "- $ \\hat{\\mathbf{y}} = f_{\\text{dec}}(\\mathbf{z}) \\in \\mathbb{R}^m $: salida reconstruida o generada\n",
        "\n",
        "\n",
        "## 1.3. El modelo\n",
        "\n",
        "El modelo completo es la composición de encoder y decoder:\n",
        "\n",
        "$$\n",
        "f_{\\text{dec}} \\circ f_{\\text{enc}}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\n",
        "$$\n",
        "\n",
        "Es decir:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = f_{\\text{dec}}(f_{\\text{enc}}(\\mathbf{x}))\n",
        "$$\n",
        "\n",
        "\n",
        "## 1.4. Función de Pérdida\n",
        "\n",
        "El modelo se entrena minimizando la pérdida entre la salida generada $ \\hat{\\mathbf{y}} $ y la salida real $ \\mathbf{y} $:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\mathcal{L}\\left(\\mathbf{y}, f_{\\text{dec}}(f_{\\text{enc}}(\\mathbf{x}))\\right)\n",
        "$$\n",
        "\n",
        "### Ejemplos comunes:\n",
        "\n",
        "- **Reconstrucción (autoencoder):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\|\\mathbf{x} - f_{\\text{dec}}(f_{\\text{enc}}(\\mathbf{x}))\\|^2\n",
        "$$\n",
        "\n",
        "- **Segmentación (por píxel):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\text{CrossEntropy}(\\mathbf{y}, \\hat{\\mathbf{y}})\n",
        "$$\n",
        "\n",
        "- **Secuencia a secuencia (traducción):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\sum_{t=1}^T \\log P(y_t \\mid \\mathbf{z}, y_{<t})\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 1.4. Optimización\n",
        "\n",
        "El entrenamiento ajusta los parámetros del modelo $ \\theta = \\{\\theta_e, \\theta_d\\} $ para minimizar la pérdida esperada:\n",
        "\n",
        "$$\n",
        "\\theta^* = \\arg \\min_{\\theta} \\ \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim \\mathcal{D}} \\left[ \\mathcal{L}(\\mathbf{y}, f_{\\text{dec}}(f_{\\text{enc}}(\\mathbf{x}))) \\right]\n",
        "$$\n",
        "\n",
        "Donde $ \\mathcal{D} $ es el conjunto de datos de entrenamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTMeH-c6QgO_"
      },
      "source": [
        "# 3. Segmentación"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: de aqui en adelante las notas hechas por el estudiante comenzaran con \"RAFAEL\" para diferenciarlas de las notas en el notebook original. Las secciones de codigo sin explicacion textual en la casilla anterior vienen directamente del notebook original sin alteraciones y no consideramos necesaria su explicacion.\n",
        "\n",
        "RAFAEL: Para esta tarea preferi usar mi maquina local la mayoria del tiempo, ya que google colab tenia un limite de unidades de computo para el free tier. Este limite se disparo varias veces, entonces configuraba el notebook para ser usado localmente (le asigne limite de memoria para el gpu)."
      ],
      "metadata": {
        "id": "pQfQNE5lws75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: Ademas, cambie el orden de algunas funciones. La descarga de datos debia ir antes que la manipulacion de las mismas. Comente la descarga de datos para no reescribirla localmente cada vez que corria todo el notebook.\n",
        "\n",
        "RAFAEL: Este notebook sirve en google colab sin problemas. Sin embargo al correrlo localmente se necesita un ambiente similar al que fue montado en labs.ucv.ai. En un jupyter de aws tuve varios problemas para correrlo y al final me rendi (capaz fue porque no escogi ubuntu como OS y no instale los paquetes en el orden correcto)."
      ],
      "metadata": {
        "id": "UHb8xeCAxFeS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFYHk8HONTLC"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: Aqui importamos varios paquetes necesarios. Es casi igual al notebook original con la diferencia de que tenemos la variable \"IS_GPU_MEMORY_LIMITED\" que usamos cuando hacemos desarrollo local con nuestra gpu de 4gb. Si tiene una GPU con mayor capacidad porfavor colocar esta variable en 0."
      ],
      "metadata": {
        "id": "aQBhRm8apQUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8WuLLxfNPS6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "from os import path\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from typing import Sequence\n",
        "from torchvision.transforms import functional as F\n",
        "import numbers\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "import dataclasses\n",
        "\n",
        "#rafael: agregue esto ya que estoy usando mi maquina local\n",
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "IS_GPU_MEMORY_LIMITED = False\n",
        "\n",
        "\n",
        "if IS_GPU_MEMORY_LIMITED:\n",
        "  # Get GPU devices\n",
        "  gpus = tf.config.list_physical_devices('GPU')\n",
        "  if gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "      tf.config.experimental.set_virtual_device_configuration(\n",
        "          gpus[0],\n",
        "          [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3000)]\n",
        "      )\n",
        "\n",
        "# rafael: agregue esto porque comence a usar mi propia maquina\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcUqE3btNhpM"
      },
      "outputs": [],
      "source": [
        "# Convert a pytorch tensor into a PIL image\n",
        "t2img = T.ToPILImage()\n",
        "# Convert a PIL image into a pytorch tensor\n",
        "img2t = T.ToTensor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTb6oEjwL9Wi"
      },
      "source": [
        "## 3.2 Descargar los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: Estos datos tardan en descargar si las tiene localmente porfavor comentar. Esta casilla fue movida mas arriba ya que abajo hay funciones que referencian las imagenes."
      ],
      "metadata": {
        "id": "PkILIX9CpiU5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUQdLh-QL9Wi"
      },
      "outputs": [],
      "source": [
        "!!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd8pUfpGMb7A"
      },
      "outputs": [],
      "source": [
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co0Sunb-L9Wi"
      },
      "source": [
        "## 3.3 Preparar los nombres de las imágenes y las máscaras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: a continuacion las secciones relacionadas con la data no se han modificado. Por lo tanto no hay comentarios del estudiante."
      ],
      "metadata": {
        "id": "i94tqs9CpxjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_unsplash_image():\n",
        "    image_path = \"stroller-unsplash.avif\"\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        url = \"https://images.unsplash.com/photo-1687017563985-a7193fcd7032\"\n",
        "        r = requests.get(url, allow_redirects=True)\n",
        "        with open(image_path, 'wb') as fp:\n",
        "            fp.write(r.content)\n",
        "        # end with\n",
        "    # end if\n",
        "\n",
        "    image_256_path = \"stroller-unsplash-256.png\"\n",
        "\n",
        "    if not os.path.exists(image_256_path):\n",
        "        img = Image.open(image_path)\n",
        "        img = img.resize((256, 256))\n",
        "        img = img.convert(\"RGB\")\n",
        "        img.save(image_256_path)\n",
        "    # end if\n",
        "\n",
        "    img = Image.open(image_256_path)\n",
        "    return img\n",
        "\n",
        "def get_pet_cat_image_and_mask():\n",
        "    img = Image.open(r\"images/Abyssinian_169.jpg\")\n",
        "    img = img.convert(\"RGB\").resize((256, 256))\n",
        "\n",
        "    mask = Image.open(r\"annotations/trimaps/Abyssinian_169.png\")\n",
        "    mask = mask.convert(\"L\").resize((256, 256))\n",
        "    mask = t2img((img2t(mask) * 255. - 1.) / 2.)\n",
        "    return img, mask\n",
        "\n",
        "img, mask = get_pet_cat_image_and_mask()\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(mask)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cGd-uXu7xDMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVp6cUABL9Wj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "img_size = (160, 160)\n",
        "num_classes = 3\n",
        "batch_size = 32\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_dir, fname)\n",
        "        for fname in os.listdir(input_dir)\n",
        "        if fname.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Número de imágenes:\", len(input_img_paths))\n",
        "\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UbUHj9YL9Wj"
      },
      "source": [
        "## 3.4 Cargando las imágenes usando Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymW7d2T9L9Wj"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from keras.utils import load_img\n",
        "from PIL import ImageOps\n",
        "\n",
        "# Display input image #7\n",
        "display(Image(filename=input_img_paths[9]))\n",
        "\n",
        "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
        "img = ImageOps.autocontrast(load_img(target_img_paths[9]))\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgohz45mL9Wj"
      },
      "source": [
        "## 3.5 Preparar dataset y batches de data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cyHQNxJL9Wj"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from tensorflow import data as tf_data\n",
        "from tensorflow import image as tf_image\n",
        "from tensorflow import io as tf_io\n",
        "\n",
        "\n",
        "def get_dataset(\n",
        "    batch_size,\n",
        "    img_size,\n",
        "    input_img_paths,\n",
        "    target_img_paths,\n",
        "    max_dataset_len=None,\n",
        "):\n",
        "    \"\"\"Returns a TF Dataset.\"\"\"\n",
        "\n",
        "    def load_img_masks(input_img_path, target_img_path):\n",
        "        input_img = tf_io.read_file(input_img_path)\n",
        "        input_img = tf_io.decode_png(input_img, channels=3)\n",
        "        input_img = tf_image.resize(input_img, img_size)\n",
        "        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
        "\n",
        "        target_img = tf_io.read_file(target_img_path)\n",
        "        target_img = tf_io.decode_png(target_img, channels=1)\n",
        "        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
        "        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n",
        "\n",
        "        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "        target_img -= 1\n",
        "        return input_img, target_img\n",
        "\n",
        "    # For faster debugging, limit the size of data\n",
        "    if max_dataset_len:\n",
        "        input_img_paths = input_img_paths[:max_dataset_len]\n",
        "        target_img_paths = target_img_paths[:max_dataset_len]\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
        "    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "    return dataset.batch(batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwo42D-bL9Wk"
      },
      "source": [
        "## 3.7 Split de validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC9MsUmwL9Wk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "val_samples = 1000\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate dataset for each split\n",
        "# Limit input files in `max_dataset_len` for faster epoch training time.\n",
        "# Remove the `max_dataset_len` arg when running with full dataset.\n",
        "train_dataset = get_dataset(\n",
        "    batch_size,\n",
        "    img_size,\n",
        "    train_input_img_paths,\n",
        "    train_target_img_paths,\n",
        "    max_dataset_len=1000,\n",
        ")\n",
        "valid_dataset = get_dataset(\n",
        "    batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
        ")\n",
        "\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezPBRKvMXhDs"
      },
      "source": [
        "## Entrenamiento con Métricas específicas del dominio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsvxwCSDXvRQ"
      },
      "source": [
        "Dependiendo del contexto en el que entrenemos modelos, a veces tiene sentido modificar la métrica de interés. Por ejemplo, en [Image segmentation metrics](https://keras.io/api/metrics/segmentation_metrics/) encontramos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aidOangkYKls"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: ya que creamos varias versiones de modelos. Hemos modificado varias secciones de codigo para que sean funciones. De esta manera sera mas facil realizar distintas acciones con los modelos."
      ],
      "metadata": {
        "id": "B_E-MhVHq31h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: la funcion de abajo simplemente es la funcion de entrenamiento."
      ],
      "metadata": {
        "id": "k8SaGDZSrNM-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyp_ftsrTgwr"
      },
      "outputs": [],
      "source": [
        "# rafael: converti esto en funcion para manejarlo mas facilmente\n",
        "def fit_model(model):\n",
        "  # Configure the model for training.\n",
        "  # We use the \"sparse\" version of categorical_crossentropy\n",
        "  # because our target data is integers.\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-4), loss=\"sparse_categorical_crossentropy\",\n",
        "      metrics=[iou_metric, dice_coefficient]\n",
        "  )\n",
        "\n",
        "  callbacks = [\n",
        "      keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\", save_best_only=True),\n",
        "      keras.callbacks.TensorBoard('./logs', update_freq=1)\n",
        "  ]\n",
        "\n",
        "  # Train the model, doing validation at the end of each epoch.\n",
        "  epochs = 50\n",
        "  history = model.fit(\n",
        "      train_dataset,\n",
        "      epochs=epochs,\n",
        "      validation_data=valid_dataset,\n",
        "      callbacks=callbacks,\n",
        "      verbose=2,\n",
        "  )\n",
        "  return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: la funcion de abajo se encarga de hacer un plot con el train history. Este tiene datos de perdida durante entrenamiento y validacion. Ademas tiene metricas de validacion del DICE e IOU."
      ],
      "metadata": {
        "id": "p7yC7DJkrVlA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGZdpxHzX6SQ"
      },
      "outputs": [],
      "source": [
        "def plot_training(history):\n",
        "  # Plot training history\n",
        "  plt.figure(figsize=(12, 4))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title('Model Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['val_dice_coefficient'], label='Validation Dice')\n",
        "  plt.plot(history.history['val_iou_metric'], label='Validation IoU')\n",
        "  plt.title('Custom metrics')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: aqui mostramos la prediccion de un modelo dado un indice. El indice representa una imagen en el dataset. Esta funcion muestra la imagen original, el target y la prediccion."
      ],
      "metadata": {
        "id": "i5es_UILr9DL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXjMqMOvRPnb"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def show_prediction(model, i=10):\n",
        "  # Generate predictions for all images in the validation set\n",
        "  val_dataset = get_dataset(\n",
        "      batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
        "  )\n",
        "  val_preds = model.predict(val_dataset)\n",
        "\n",
        "  def display_mask(i):\n",
        "      \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "      mask = np.argmax(val_preds[i], axis=-1)\n",
        "      mask = np.expand_dims(mask, axis=-1)\n",
        "      img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
        "      display(img)\n",
        "\n",
        "  # Display input image\n",
        "  print(\"HERE\",Image)\n",
        "  display(Image(filename=val_input_img_paths[i]))\n",
        "\n",
        "  # Display ground-truth target mask\n",
        "  img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
        "  display(img)\n",
        "\n",
        "  # Display mask predicted by our model\n",
        "  display_mask(i)  # Note that the model only sees inputs at 150x150."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: estas funciones no estaban en el notebook original. Aqui es necesario que exista un directorio llamado \"models\". La primera funcion guarda un modelo y su historial de entrenamiento dado un prefijo que le asigna el usuario.\n",
        "\n",
        "La segunda funcion trae devuelta esa informacion dado el prefijo. Esta no es usada en el resto del notebook, se asume que el mismo sera corrido de 0. Igual se guardan los modelos localmente"
      ],
      "metadata": {
        "id": "43z9Z2rasm4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def save_experiment(model, history, prefix):\n",
        "    experiment_dir = f\"models/{prefix}\"\n",
        "    os.makedirs(experiment_dir, exist_ok=True)\n",
        "\n",
        "    model_path = os.path.join(experiment_dir, \"model.keras\")\n",
        "    model.save(model_path)\n",
        "\n",
        "    history_pickle_path = os.path.join(experiment_dir, \"history.pkl\")\n",
        "    with open(history_pickle_path, 'wb') as f:\n",
        "        pickle.dump(history.history, f)\n",
        "\n",
        "    return experiment_dir\n",
        "\n",
        "\n",
        "def load_experiment(prefix):\n",
        "    experiment_dir = f\"experiments/{prefix}\"\n",
        "\n",
        "    if not os.path.exists(experiment_dir):\n",
        "        raise FileNotFoundError(f\"Experiment directory not found: {experiment_dir}\")\n",
        "\n",
        "    model = None\n",
        "    history = None\n",
        "    history_pickle_path = os.path.join(experiment_dir, \"history.pkl\")\n",
        "    with open(history_pickle_path, 'rb') as f:\n",
        "        history = pickle.load(f)\n",
        "\n",
        "    return model, history, experiment_dir"
      ],
      "metadata": {
        "id": "KXGpq7hHybR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZsIS6quL9Wj"
      },
      "source": [
        "## Modelos\n",
        "\n",
        "RAFAEL: a continuacion los distintos modelos y el original. Se puede comparar tanto el historial de entrenamiento como una unica prediccion."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original"
      ],
      "metadata": {
        "id": "dcSrj3DUU-NK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC6oOfPcL9Wj"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: Algo que tuve bastante presente durante la creacion de este notebook es la cantidad de parametros totales en cada modelo. Debido a que la mayoria del desarrollo ser realizo con tiempo y un GPU local, evite crear modelos por encima de los 2.3M de parametros. Por encima de este numero mi GPU (aun con la configuracion para solo gastar 3Gb) no era capaz de entrenarlo."
      ],
      "metadata": {
        "id": "awnS1Eu0tJgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_history = fit_model(model)\n",
        "save_experiment(model,train_history, \"original\")"
      ],
      "metadata": {
        "id": "Z0xwJ6dph4zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL: segun observamos, el DICE (el cual consideramos una de las metricas mas importantes), no llega a 0.9. La metrica IOU supera el 0.8. Durante el entrenamiento estas metricas parecen mantenerse fuertes y luego bajan y reducen su estabilidad.\n",
        "\n",
        "La perdida en validacion parece aumentar bastante y luego bajar y regularse. Al tratar de predecir la segmentacion del pug, evidentemente tenemos problemas. Ni siquiera se obtienen bordes suaves."
      ],
      "metadata": {
        "id": "UmcVmT0humj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(train_history)"
      ],
      "metadata": {
        "id": "zWrQc8xDiJf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_prediction(model,10)"
      ],
      "metadata": {
        "id": "ITRtDmVIug0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V1 - Eliminando la suma de capas anteriores"
      ],
      "metadata": {
        "id": "Patd3t7QVC3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAFAEL:"
      ],
      "metadata": {
        "id": "bK8yyNLcvTds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "modelv1 = get_model(img_size, num_classes)\n",
        "modelv1.summary()"
      ],
      "metadata": {
        "id": "clZYivJ-VCPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_history = fit_model(modelv1)\n",
        "save_experiment(model,train_history, \"v1\")"
      ],
      "metadata": {
        "id": "pFgIYIRRpRhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(train_history)"
      ],
      "metadata": {
        "id": "wCwHQ9BdvCpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_prediction(modelv1,10)"
      ],
      "metadata": {
        "id": "xni933YQwqA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Exhr07h018az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V2 - Cambiando la estructura de la red"
      ],
      "metadata": {
        "id": "-uyDB21318m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "    x = inputs\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [16, 32, 64, 128]:\n",
        "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "    for filters in [128, 64, 32, 16]:\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "AtOOp9zi18m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_history = fit_model(model)\n",
        "save_experiment(model,train_history, \"v2\")"
      ],
      "metadata": {
        "id": "j6W1--t_18m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(train_history)"
      ],
      "metadata": {
        "id": "J-1lQQ4Q18m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_prediction(model,10)"
      ],
      "metadata": {
        "id": "-SvsZi6O18m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V3 - Agregando convoluciones Depthwise"
      ],
      "metadata": {
        "id": "fzMkJVUEOyH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "    x = inputs\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    for filter_multiplier in [10, 2, 2, 2]:\n",
        "        # Rafael: depth_multiplier multiplica la cantidad de filtros/canales anteriores.\n",
        "        # esto indica que pasaremos de 3 canales a 30, 60, 120 y 240\n",
        "        x = layers.DepthwiseConv2D(3, depth_multiplier=filter_multiplier, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        # rafael: aqui solo aplicamos depth_multiplier con 1. Para mantener la cantidad de canales\n",
        "        x = layers.DepthwiseConv2D(3, depth_multiplier=1, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "    for filters in [240, 120, 60, 30]:\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "eTQrMbk_OyH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_history = fit_model(model)\n",
        "save_experiment(model,train_history, \"v3\")"
      ],
      "metadata": {
        "id": "Jrb7LaMnOyIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(train_history)"
      ],
      "metadata": {
        "id": "sDOOkP9zOyIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_prediction(model,10)"
      ],
      "metadata": {
        "id": "CxdX2YDhOyIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V4 - Modelo original con depthwise"
      ],
      "metadata": {
        "id": "VzGwXDjfXrGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "\n",
        "    x = layers.DepthwiseConv2D(3, depth_multiplier=10, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [2, 2, 2]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.DepthwiseConv2D(3, depth_multiplier=filters, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.DepthwiseConv2D(3, depth_multiplier=1, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.DepthwiseConv2D(1, depth_multiplier=filters, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [240, 120, 60, 30]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "iOrImxSKXrGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_history = fit_model(model)\n",
        "save_experiment(model,train_history, \"v4\")"
      ],
      "metadata": {
        "id": "EVZACYykXrGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(train_history)"
      ],
      "metadata": {
        "id": "mOp_Lw8RXrGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_prediction(model,10)"
      ],
      "metadata": {
        "id": "c7ichpZtXrGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V5 - Sin depthwise y mas capas y canales"
      ],
      "metadata": {
        "id": "eCny7BM7pPxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "    x = inputs\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Rafael. esto antes era 128, 64, 64, 128. A causa de que explotaba la memoria del gpu con el primer 128, se cambio a 64\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [32, 64, 128, 128, 128]:\n",
        "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "    for filters in  [128, 128, 128, 64, 32]:\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Ssa12bfapPxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_history = fit_model(model)\n",
        "save_experiment(model,train_history, \"v5\")"
      ],
      "metadata": {
        "id": "yA23IA9ppPxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(train_history)"
      ],
      "metadata": {
        "id": "GGpz-yD1pPxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_prediction(model,10)"
      ],
      "metadata": {
        "id": "udACEddPpPxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V6 - Con dropout"
      ],
      "metadata": {
        "id": "0uwFY94wkdBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "    x = inputs\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    dropout_rate = 0.1\n",
        "    # Rafael. esto antes era 128, 64, 64, 128. A causa de que explotaba la memoria del gpu con el primer 128, se cambio a 64\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [32, 64, 128, 180]:\n",
        "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SpatialDropout2D(dropout_rate)(x)\n",
        "\n",
        "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "    for filters in  [180, 128, 64, 32]:\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SpatialDropout2D(dropout_rate)(x)\n",
        "\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "5D9Qd8HCkdBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_history = fit_model(model)\n",
        "save_experiment(model,train_history, \"v6\")"
      ],
      "metadata": {
        "id": "LujVGfo2kdBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(train_history)"
      ],
      "metadata": {
        "id": "4_V1DsRlkdBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_prediction(model,10)"
      ],
      "metadata": {
        "id": "DqMabA0zkdBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Zb2eOzZ8s9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V7 - Hasta 512 canales y con suma previa\n",
        "\n",
        "Usando v2. que es el que mejor performance tuvo en las metricas"
      ],
      "metadata": {
        "id": "n5YY5-edGCbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "    x = inputs\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [32, 64, 128, 512]:\n",
        "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.SpatialDropout2D(0.1)(x)\n",
        "\n",
        "        x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "    for filters in [512, 128, 64, 32]:\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SpatialDropout2D(0.1)(x)\n",
        "\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cOeyPeIOGCbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_history = fit_model(model)\n",
        "save_experiment(model,train_history, \"v7\")"
      ],
      "metadata": {
        "id": "GaXtXoBCGCbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(train_history)"
      ],
      "metadata": {
        "id": "viDxFcVpGCbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_prediction(model,10)"
      ],
      "metadata": {
        "id": "POTpy5pZGCbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6CxXoscjGCbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "oxford_pets_image_segmentation",
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}