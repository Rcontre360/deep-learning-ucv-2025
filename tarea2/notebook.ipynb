{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWo8FaiML9Wh"
      },
      "source": [
        "<div><center><img src=\"https://ml.ucv.ai/logo.png\\\" width=150\"/> </center></div>\n",
        "\n",
        "# 04 Segmentación de imágenes con redes U-Net\n",
        "\n",
        "\n",
        "Basado en:\n",
        "\n",
        "- [Efficient Image Segmentation](https://medium.com/data-science/efficient-image-segmentation-using-pytorch-part-3-3534cf04fb89)\n",
        "- [Image segmentation with a U-Net-like architecture](https://keras.io/examples/vision/oxford_pets_image_segmentation/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvG-a4nAOTqS"
      },
      "source": [
        "# 1. Arquitecturas Encoder-Decoder (Teoría y Fórmulas)\n",
        "\n",
        "Un modelo **encoder-decoder** transforma una entrada $ \\mathbf{x} \\in \\mathbb{R}^n $ en una salida $ \\mathbf{y} \\in \\mathbb{R}^m $ mediante una representación latente $ \\mathbf{z} \\in \\mathbb{R}^d $, donde normalmente $ d < n $.\n",
        "\n",
        "## 1.1 Encoder\n",
        "\n",
        "El **encoder** convierte la entrada $ \\mathbf{x} $ en una representación latente $ \\mathbf{z} $:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = f_{\\text{enc}}(\\mathbf{x}; \\theta_e)\n",
        "$$\n",
        "\n",
        "- $ f_{\\text{enc}} $: red neuronal (CNN, RNN, MLP, etc.)\n",
        "- $ \\theta_e $: parámetros del encoder\n",
        "- $ \\mathbf{z} $: vector latente comprimido\n",
        "\n",
        "Si el encoder tiene múltiples capas:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1(\\mathbf{x})\n",
        "$$\n",
        "\n",
        "En términos de dominio y codominio:\n",
        "\n",
        "$$\n",
        "f_{\\text{enc}}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^d\n",
        "$$\n",
        "\n",
        "- $ \\mathbf{x} \\in \\mathbb{R}^n $: entrada original\n",
        "- $ \\mathbf{z} = f_{\\text{enc}}(\\mathbf{x}) \\in \\mathbb{R}^d $: representación latente\n",
        "- Típicamente $ d < n $ para compresión\n",
        "\n",
        "\n",
        "## 1.2. Decoder\n",
        "\n",
        "El **decoder** reconstruye o genera la salida a partir de $ \\mathbf{z} $:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = f_{\\text{dec}}(\\mathbf{z}; \\theta_d)\n",
        "$$\n",
        "\n",
        "- $ f_{\\text{dec}} $: red decodificadora\n",
        "- $ \\theta_d $: parámetros del decoder\n",
        "- $ \\hat{\\mathbf{y}} $: salida generada o reconstruida\n",
        "\n",
        "Reconstruye o genera la salida desde el espacio latente:\n",
        "\n",
        "$$\n",
        "f_{\\text{dec}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^m\n",
        "$$\n",
        "\n",
        "- $ \\mathbf{z} \\in \\mathbb{R}^d $: vector latente\n",
        "- $ \\hat{\\mathbf{y}} = f_{\\text{dec}}(\\mathbf{z}) \\in \\mathbb{R}^m $: salida reconstruida o generada\n",
        "\n",
        "\n",
        "## 1.3. El modelo\n",
        "\n",
        "El modelo completo es la composición de encoder y decoder:\n",
        "\n",
        "$$\n",
        "f_{\\text{dec}} \\circ f_{\\text{enc}}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\n",
        "$$\n",
        "\n",
        "Es decir:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = f_{\\text{dec}}(f_{\\text{enc}}(\\mathbf{x}))\n",
        "$$\n",
        "\n",
        "\n",
        "## 1.4. Función de Pérdida\n",
        "\n",
        "El modelo se entrena minimizando la pérdida entre la salida generada $ \\hat{\\mathbf{y}} $ y la salida real $ \\mathbf{y} $:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\mathcal{L}\\left(\\mathbf{y}, f_{\\text{dec}}(f_{\\text{enc}}(\\mathbf{x}))\\right)\n",
        "$$\n",
        "\n",
        "### Ejemplos comunes:\n",
        "\n",
        "- **Reconstrucción (autoencoder):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\|\\mathbf{x} - f_{\\text{dec}}(f_{\\text{enc}}(\\mathbf{x}))\\|^2\n",
        "$$\n",
        "\n",
        "- **Segmentación (por píxel):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\text{CrossEntropy}(\\mathbf{y}, \\hat{\\mathbf{y}})\n",
        "$$\n",
        "\n",
        "- **Secuencia a secuencia (traducción):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\sum_{t=1}^T \\log P(y_t \\mid \\mathbf{z}, y_{<t})\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## 1.4. Optimización\n",
        "\n",
        "El entrenamiento ajusta los parámetros del modelo $ \\theta = \\{\\theta_e, \\theta_d\\} $ para minimizar la pérdida esperada:\n",
        "\n",
        "$$\n",
        "\\theta^* = \\arg \\min_{\\theta} \\ \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim \\mathcal{D}} \\left[ \\mathcal{L}(\\mathbf{y}, f_{\\text{dec}}(f_{\\text{enc}}(\\mathbf{x}))) \\right]\n",
        "$$\n",
        "\n",
        "Donde $ \\mathcal{D} $ es el conjunto de datos de entrenamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTMeH-c6QgO_"
      },
      "source": [
        "# 3. Segmentación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFYHk8HONTLC",
        "outputId": "888e39bb-c099-41e7-b7b5-e40f770d4b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in ./venv/lib/python3.12/site-packages (1.8.0)\r\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i8WuLLxfNPS6",
        "outputId": "c2113790-3537-4ee3-cb9a-f4191714cb6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot set memory growth on device when virtual devices configured",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m gpus = tf.config.list_physical_devices(\u001b[33m'\u001b[39m\u001b[33mGPU\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gpus:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexperimental\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_memory_growth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     tf.config.experimental.set_virtual_device_configuration(\n\u001b[32m     27\u001b[39m         gpus[\u001b[32m0\u001b[39m],\n\u001b[32m     28\u001b[39m         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=\u001b[32m3096\u001b[39m)]\n\u001b[32m     29\u001b[39m     )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# rafael: agregue esto porque comence a usar mi propia maquina\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ucv/DL/venv/lib/python3.12/site-packages/tensorflow/python/framework/config.py:754\u001b[39m, in \u001b[36mset_memory_growth\u001b[39m\u001b[34m(device, enable)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mconfig.experimental.set_memory_growth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_memory_growth\u001b[39m(device, enable):\n\u001b[32m    731\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Set if memory growth should be enabled for a `PhysicalDevice`.\u001b[39;00m\n\u001b[32m    732\u001b[39m \n\u001b[32m    733\u001b[39m \u001b[33;03m  If memory growth is enabled for a `PhysicalDevice`, the runtime initialization\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    752\u001b[39m \u001b[33;03m    RuntimeError: Runtime is already initialized.\u001b[39;00m\n\u001b[32m    753\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m   \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_memory_growth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ucv/DL/venv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1985\u001b[39m, in \u001b[36mContext.set_memory_growth\u001b[39m\u001b[34m(self, dev, enable)\u001b[39m\n\u001b[32m   1982\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnrecognized device: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mrepr\u001b[39m(dev))\n\u001b[32m   1984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dev \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._virtual_device_map:\n\u001b[32m-> \u001b[39m\u001b[32m1985\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1986\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mCannot set memory growth on device when virtual devices configured\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1987\u001b[39m   )\n\u001b[32m   1989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dev.device_type != \u001b[33m\"\u001b[39m\u001b[33mGPU\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dev \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pluggable_devices:\n\u001b[32m   1990\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1991\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mCannot set memory growth on non-GPU and non-Pluggable devices\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1992\u001b[39m   )\n",
            "\u001b[31mValueError\u001b[39m: Cannot set memory growth on device when virtual devices configured"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "from os import path\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from typing import Sequence\n",
        "from torchvision.transforms import functional as F\n",
        "import numbers\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "import dataclasses\n",
        "from torchinfo import summary\n",
        "\n",
        "#rafael: agregue esto ya que estoy usando mi maquina local\n",
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Get GPU devices\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3096)]\n",
        "    )\n",
        "\n",
        "# rafael: agregue esto porque comence a usar mi propia maquina\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcUqE3btNhpM"
      },
      "outputs": [],
      "source": [
        "# Convert a pytorch tensor into a PIL image\n",
        "t2img = T.ToPILImage()\n",
        "# Convert a PIL image into a pytorch tensor\n",
        "img2t = T.ToTensor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkCvM4JCNMKJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_unsplash_image():\n",
        "    image_path = \"stroller-unsplash.avif\"\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        url = \"https://images.unsplash.com/photo-1687017563985-a7193fcd7032\"\n",
        "        r = requests.get(url, allow_redirects=True)\n",
        "        with open(image_path, 'wb') as fp:\n",
        "            fp.write(r.content)\n",
        "        # end with\n",
        "    # end if\n",
        "\n",
        "    image_256_path = \"stroller-unsplash-256.png\"\n",
        "\n",
        "    if not os.path.exists(image_256_path):\n",
        "        img = Image.open(image_path)\n",
        "        img = img.resize((256, 256))\n",
        "        img = img.convert(\"RGB\")\n",
        "        img.save(image_256_path)\n",
        "    # end if\n",
        "\n",
        "    img = Image.open(image_256_path)\n",
        "    return img\n",
        "\n",
        "def get_pet_cat_image_and_mask():\n",
        "    img = Image.open(r\"images/Abyssinian_169.jpg\")\n",
        "    img = img.convert(\"RGB\").resize((256, 256))\n",
        "\n",
        "    mask = Image.open(r\"annotations/trimaps/Abyssinian_169.png\")\n",
        "    mask = mask.convert(\"L\").resize((256, 256))\n",
        "    mask = t2img((img2t(mask) * 255. - 1.) / 2.)\n",
        "    return img, mask\n",
        "\n",
        "img, mask = get_pet_cat_image_and_mask()\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(mask)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTb6oEjwL9Wi"
      },
      "source": [
        "## 3.2 Descargar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUQdLh-QL9Wi"
      },
      "outputs": [],
      "source": [
        "#!!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "#!!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd8pUfpGMb7A"
      },
      "outputs": [],
      "source": [
        "#!tar -xf images.tar.gz\n",
        "#!tar -xf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co0Sunb-L9Wi"
      },
      "source": [
        "## 3.3 Preparar los nombres de las imágenes y las máscaras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVp6cUABL9Wj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "img_size = (160, 160)\n",
        "num_classes = 3\n",
        "batch_size = 32\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_dir, fname)\n",
        "        for fname in os.listdir(input_dir)\n",
        "        if fname.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Número de imágenes:\", len(input_img_paths))\n",
        "\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UbUHj9YL9Wj"
      },
      "source": [
        "## 3.4 Cargando las imágenes usando Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymW7d2T9L9Wj"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from keras.utils import load_img\n",
        "from PIL import ImageOps\n",
        "\n",
        "# Display input image #7\n",
        "display(Image(filename=input_img_paths[9]))\n",
        "\n",
        "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
        "img = ImageOps.autocontrast(load_img(target_img_paths[9]))\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgohz45mL9Wj"
      },
      "source": [
        "## 3.5 Preparar dataset y batches de data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cyHQNxJL9Wj"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from tensorflow import data as tf_data\n",
        "from tensorflow import image as tf_image\n",
        "from tensorflow import io as tf_io\n",
        "\n",
        "\n",
        "def get_dataset(\n",
        "    batch_size,\n",
        "    img_size,\n",
        "    input_img_paths,\n",
        "    target_img_paths,\n",
        "    max_dataset_len=None,\n",
        "):\n",
        "    \"\"\"Returns a TF Dataset.\"\"\"\n",
        "\n",
        "    def load_img_masks(input_img_path, target_img_path):\n",
        "        input_img = tf_io.read_file(input_img_path)\n",
        "        input_img = tf_io.decode_png(input_img, channels=3)\n",
        "        input_img = tf_image.resize(input_img, img_size)\n",
        "        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
        "\n",
        "        target_img = tf_io.read_file(target_img_path)\n",
        "        target_img = tf_io.decode_png(target_img, channels=1)\n",
        "        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
        "        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n",
        "\n",
        "        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "        target_img -= 1\n",
        "        return input_img, target_img\n",
        "\n",
        "    # For faster debugging, limit the size of data\n",
        "    if max_dataset_len:\n",
        "        input_img_paths = input_img_paths[:max_dataset_len]\n",
        "        target_img_paths = target_img_paths[:max_dataset_len]\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
        "    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "    return dataset.batch(batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwo42D-bL9Wk"
      },
      "source": [
        "## 3.7 Split de validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC9MsUmwL9Wk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "val_samples = 1000\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate dataset for each split\n",
        "# Limit input files in `max_dataset_len` for faster epoch training time.\n",
        "# Remove the `max_dataset_len` arg when running with full dataset.\n",
        "train_dataset = get_dataset(\n",
        "    batch_size,\n",
        "    img_size,\n",
        "    train_input_img_paths,\n",
        "    train_target_img_paths,\n",
        "    max_dataset_len=1000,\n",
        ")\n",
        "valid_dataset = get_dataset(\n",
        "    batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezPBRKvMXhDs"
      },
      "source": [
        "## Entrenamiento con Métricas específicas del dominio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsvxwCSDXvRQ"
      },
      "source": [
        "Dependiendo del contexto en el que entrenemos modelos, a veces tiene sentido modificar la métrica de interés. Por ejemplo, en [Image segmentation metrics](https://keras.io/api/metrics/segmentation_metrics/) encontramos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aidOangkYKls"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyp_ftsrTgwr"
      },
      "outputs": [],
      "source": [
        "# rafael: converti esto en funcion para manejarlo mas facilmente\n",
        "def fit_model(model):\n",
        "  # Configure the model for training.\n",
        "  # We use the \"sparse\" version of categorical_crossentropy\n",
        "  # because our target data is integers.\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-4), loss=\"sparse_categorical_crossentropy\",\n",
        "      metrics=[iou_metric, dice_coefficient]\n",
        "  )\n",
        "\n",
        "  callbacks = [\n",
        "      keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\", save_best_only=True),\n",
        "      keras.callbacks.TensorBoard('./logs', update_freq=1)\n",
        "  ]\n",
        "\n",
        "  # Train the model, doing validation at the end of each epoch.\n",
        "  epochs = 50\n",
        "  history = model.fit(\n",
        "      train_dataset,\n",
        "      epochs=epochs,\n",
        "      validation_data=valid_dataset,\n",
        "      callbacks=callbacks,\n",
        "      verbose=2,\n",
        "  )\n",
        "  return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGZdpxHzX6SQ"
      },
      "outputs": [],
      "source": [
        "def plot_training(history):\n",
        "  # Plot training history\n",
        "  plt.figure(figsize=(12, 4))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title('Model Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['val_dice_coefficient'], label='Validation Dice')\n",
        "  plt.plot(history.history['val_iou_metric'], label='Validation IoU')\n",
        "  plt.title('Custom metrics')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXjMqMOvRPnb"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "def show_prediction(model):\n",
        "  # Generate predictions for all images in the validation set\n",
        "  val_dataset = get_dataset(\n",
        "      batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
        "  )\n",
        "  val_preds = model.predict(val_dataset)\n",
        "\n",
        "\n",
        "  def display_mask(i):\n",
        "      \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "      mask = np.argmax(val_preds[i], axis=-1)\n",
        "      mask = np.expand_dims(mask, axis=-1)\n",
        "      img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
        "      display(img)\n",
        "\n",
        "\n",
        "  # Display results for validation image #10\n",
        "  i = 10\n",
        "\n",
        "  # Display input image\n",
        "  display(Image(filename=val_input_img_paths[i]))\n",
        "\n",
        "  # Display ground-truth target mask\n",
        "  img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
        "  display(img)\n",
        "\n",
        "  # Display mask predicted by our model\n",
        "  display_mask(i)  # Note that the model only sees inputs at 150x150."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZsIS6quL9Wj"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original"
      ],
      "metadata": {
        "id": "dcSrj3DUU-NK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC6oOfPcL9Wj"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_history = fit_model(model)"
      ],
      "metadata": {
        "id": "Z0xwJ6dph4zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(train_history)\n",
        "show_prediction(model)"
      ],
      "metadata": {
        "id": "zWrQc8xDiJf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V1 -"
      ],
      "metadata": {
        "id": "Patd3t7QVC3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "clZYivJ-VCPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "oxford_pets_image_segmentation",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}