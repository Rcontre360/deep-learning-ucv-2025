{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 12527845,
          "sourceType": "datasetVersion",
          "datasetId": 7632487
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div><center><img src=\"https://ml.ucv.ai/logo.png\\\" width=150\"/> </center></div>\n",
        "\n",
        "# Tarea 3: Transformers y Stacked AutoEncoders\n",
        "\n",
        "Fuentes:\n",
        "\n",
        "- [BRISC](https://www.kaggle.com/datasets/briscdataset/brisc2025)\n"
      ],
      "metadata": {
        "id": "IgPgbEIMulRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descargando datos"
      ],
      "metadata": {
        "id": "iPPxM671u5hQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos la librer√≠a kagglehub para descargar los datos. Por defecto, los datos son descargados en la `cache` de `.root` necesitamos moverlos a nuestro directorio luego de descargar."
      ],
      "metadata": {
        "id": "cgTsKKO8u7V8"
      }
    },
    {
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download('briscdataset/brisc2025')\n",
        "\n",
        "print('Descarga de datos completa.', path)"
      ],
      "metadata": {
        "id": "zf2SVYO1tjK_"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {path}"
      ],
      "metadata": {
        "id": "T325lNU8uSYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quitar el anterior y usar copy para evitar descargar varias veces en un ambiente persistente (local)\n",
        "!rm -r ./briscdataset\n",
        "!cp -r {path}/brisc2025 ./briscdataset"
      ],
      "metadata": {
        "id": "C-_ng-Zkt2vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refresquen el directorio de Google Colab"
      ],
      "metadata": {
        "id": "3YzOGZLlvcRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† BRISC 2025 Dataset Exploration\n",
        "\n",
        "BRISC es un conjunto de datos de resonancias magn√©ticas de alta calidad, anotado por expertos, dise√±ado para la segmentaci√≥n y clasificaci√≥n de tumores cerebrales. Aborda limitaciones comunes en los conjuntos de datos existentes (p. ej., BraTS, Figshare), como el desequilibrio de clases, la focalizaci√≥n tumoral limitada y las inconsistencias en la anotaci√≥n.\n",
        "\n",
        "Esta secci√≥n provee m√©todos para entender los datos del dataset **BRISC 2025**. Est√° modificado para ser usado en `Google Colab` de un tutorial que funcionaba en `Kaggle`\n",
        "\n",
        "1. Setup & Imports\n",
        "2. Estructura del directorio  \n",
        "3. N√∫mero de archivos y distribuciones\n",
        "4. Representaci√≥n gr√°fica de distribuciones\n",
        "5. Num√©ro de clasificaci√≥n por plano\n",
        "6. Visualizaci√≥n de im√°genes de muestra y m√°scaras\n",
        "7. Histogramas de intensidad de p√≠xeles\n",
        "8. An√°lisis de metadatos de nombres de archivo\n",
        "9. Ejemplo de superposici√≥n de m√°scaras\n",
        "10. Cuadr√≠cula aleatoria de muestras de clasificaci√≥n\n"
      ],
      "metadata": {
        "id": "OmHgmaEqtjLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è 1. Setup & Imports\n",
        "\n",
        "Librer√≠as usadas por el tutorial. La mayor√≠a conocidas:\n",
        "- Archivo y manejo de directorios (`os`, `glob`)\n",
        "- Datos (`pandas`, `numpy`)\n",
        "- Procesamiento de im√°genes (`PIL.Image`)\n",
        "- Visualizaci√≥n (`matplotlib`)\n",
        "\n",
        "Tomen en cuenta que usamos el directorio base luego de copiar en Google Colab."
      ],
      "metadata": {
        "id": "sOMB19JbtjLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[1] ‚Äî Setup & Imports\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Render plots inline\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "BASE_DIR = \"briscdataset/\"\n",
        "print(\"‚úÖ Final BASE_DIR =\", BASE_DIR)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T21:28:26.717657Z",
          "iopub.execute_input": "2025-06-24T21:28:26.718014Z",
          "iopub.status.idle": "2025-06-24T21:28:27.117966Z",
          "shell.execute_reply.started": "2025-06-24T21:28:26.717987Z",
          "shell.execute_reply": "2025-06-24T21:28:27.116857Z"
        },
        "id": "F9j3BINbtjLC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "No cambien el c√≥digo abajo, si copiaron bien los archivos deber√≠a verse como en la imagen"
      ],
      "metadata": {
        "id": "hmPJ8IVkxdSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {BASE_DIR}"
      ],
      "metadata": {
        "id": "UUcnMC6nwOlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÇ 2. Estructura del directorio\n",
        "\n",
        "Navegamos recursivamente en el `BASE_DIR` (profundidad=2)"
      ],
      "metadata": {
        "id": "qKKwXRkXtjLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[2] ‚Äî Directory Tree\n",
        "def walk_dir(base, max_depth=2):\n",
        "    for root, dirs, _ in os.walk(base):\n",
        "        depth = root.replace(base, \"\").count(os.sep)\n",
        "        if depth <= max_depth:\n",
        "            indent = \"  \" * depth\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            for d in dirs:\n",
        "                print(f\"{indent}  {d}/\")\n",
        "\n",
        "print(\"## BRISC2025 Folder Layout:\")\n",
        "walk_dir(BASE_DIR, max_depth=2)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T21:28:31.502117Z",
          "iopub.execute_input": "2025-06-24T21:28:31.50272Z",
          "iopub.status.idle": "2025-06-24T21:28:52.913623Z",
          "shell.execute_reply.started": "2025-06-24T21:28:31.502685Z",
          "shell.execute_reply": "2025-06-24T21:28:52.912654Z"
        },
        "id": "yepEnCoEtjLD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä 3. Distribuci√≥n de archivos\n",
        "\n",
        "Contando cada imagen `.jpg` en cada uno de los split/clases.\n"
      ],
      "metadata": {
        "id": "3tHPOeFQtjLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File Count Logic (no need to change if BASE_DIR is fixed)\n",
        "def count_patterns(patterns):\n",
        "    return sum(len(glob.glob(p)) for p in patterns)\n",
        "\n",
        "# Classification\n",
        "splits = [\"train\", \"test\"]\n",
        "classes = [\"glioma\", \"meningioma\", \"pituitary\", \"no_tumor\"]\n",
        "cls_records = []\n",
        "for sp in splits:\n",
        "    total = 0\n",
        "    for cls in classes:\n",
        "        pat = os.path.join(BASE_DIR, \"classification_task\", sp, cls, \"*.jpg\")\n",
        "        cnt = count_patterns([pat])\n",
        "        cls_records.append({'Split': sp, 'Class': cls, 'Count': cnt})\n",
        "        total += cnt\n",
        "    cls_records.append({'Split': sp, 'Class': 'Total', 'Count': total})\n",
        "df_cls = pd.DataFrame(cls_records)\n",
        "\n",
        "# Segmentation\n",
        "types_ = [\"images\", \"masks\"]\n",
        "seg_records = []\n",
        "for sp in splits:\n",
        "    for tp in types_:\n",
        "        pats = [\n",
        "            os.path.join(BASE_DIR, \"segmentation_task\", sp, tp, \"*.jpg\"),\n",
        "            os.path.join(BASE_DIR, \"segmentation_task\", sp, tp, \"*.png\")\n",
        "        ]\n",
        "        cnt = count_patterns(pats)\n",
        "        seg_records.append({'Split': sp, 'Type': tp, 'Count': cnt})\n",
        "df_seg = pd.DataFrame(seg_records)\n",
        "\n",
        "# Display\n",
        "print(\"### Classification Counts\")\n",
        "display(df_cls)\n",
        "print(\"### Segmentation Counts\")\n",
        "display(df_seg)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T21:29:05.750609Z",
          "iopub.execute_input": "2025-06-24T21:29:05.750991Z",
          "iopub.status.idle": "2025-06-24T21:29:05.855127Z",
          "shell.execute_reply.started": "2025-06-24T21:29:05.750959Z",
          "shell.execute_reply": "2025-06-24T21:29:05.854199Z"
        },
        "id": "fH_D6CxLtjLD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä 4. Gr√°ficos por distribuciones\n",
        "\n",
        "Visualizaci√≥n del conjunto de datos\n"
      ],
      "metadata": {
        "id": "-saEP_sktjLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[4] ‚Äî Distribution Plots\n",
        "for sp in splits:\n",
        "    sub = df_cls[df_cls.Split==sp]\n",
        "    plt.figure(figsize=(5,3))\n",
        "    plt.bar(sub.Class, sub.Count, edgecolor='k')\n",
        "    plt.title(f\"{sp.title()} Classification Distribution\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "for sp in splits:\n",
        "    sub = df_seg[df_seg.Split==sp]\n",
        "    plt.figure(figsize=(5,3))\n",
        "    plt.bar(sub.Type, sub.Count, edgecolor='k')\n",
        "    plt.title(f\"{sp.title()} Segmentation Distribution\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T21:29:22.651816Z",
          "iopub.execute_input": "2025-06-24T21:29:22.652167Z",
          "iopub.status.idle": "2025-06-24T21:29:23.278501Z",
          "shell.execute_reply.started": "2025-06-24T21:29:22.652141Z",
          "shell.execute_reply": "2025-06-24T21:29:23.277417Z"
        },
        "id": "0a7kWNR-tjLD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà 5. Clasificaci√≥n por planos\n",
        "\n",
        "Resumen an√°tomico dependiendo de los planos:\n",
        "\n",
        "1. Ax: Axial\n",
        "1. co: Coronal\n",
        "1. sa: Sagittal"
      ],
      "metadata": {
        "id": "KBLmgCBvtjLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[5] ‚Äî Per‚ÄëPlane Counts\n",
        "planes = [\"ax\", \"co\", \"sa\"]\n",
        "plane_records = []\n",
        "\n",
        "for sp in splits:\n",
        "    for cls in classes:\n",
        "        for pl in planes:\n",
        "            pat = os.path.join(\n",
        "                BASE_DIR, \"classification_task\", sp, cls, f\"*_{pl}_t1.jpg\"\n",
        "            )\n",
        "            cnt = count_patterns([pat])\n",
        "            plane_records.append({'Split': sp, 'Class': cls, 'Plane': pl, 'Count': cnt})\n",
        "\n",
        "df_plane = pd.DataFrame(plane_records)\n",
        "print(\"### Per‚ÄëPlane Counts Pivot\")\n",
        "display(df_plane.pivot_table(index='Plane', columns=['Split','Class'], values='Count'))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T21:31:31.67246Z",
          "iopub.execute_input": "2025-06-24T21:31:31.673406Z",
          "iopub.status.idle": "2025-06-24T21:31:31.766881Z",
          "shell.execute_reply.started": "2025-06-24T21:31:31.673375Z",
          "shell.execute_reply": "2025-06-24T21:31:31.766075Z"
        },
        "id": "V9SvtWL3tjLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üñºÔ∏è 6. Imagen de ejemplo y m√°scaras\n",
        "\n",
        "Ejemplo de una imagen de clasificaci√≥n con su imagen de segmentaci√≥n."
      ],
      "metadata": {
        "id": "ilMNFAX5tjLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[6] ‚Äî Sample Visualization\n",
        "def first_file(folder, exts):\n",
        "    for ext in exts:\n",
        "        fl = glob.glob(os.path.join(folder, f\"*.{ext}\"))\n",
        "        if fl: return fl[0]\n",
        "    return None\n",
        "\n",
        "img_path  = first_file(os.path.join(BASE_DIR, \"classification_task/train/glioma\"), [\"jpg\"])\n",
        "mask_path = first_file(os.path.join(BASE_DIR, \"segmentation_task/train/masks\"), [\"png\",\"jpg\"])\n",
        "\n",
        "img  = Image.open(img_path)\n",
        "mask = Image.open(mask_path)\n",
        "\n",
        "fig, axes = plt.subplots(1,2, figsize=(10,5))\n",
        "axes[0].imshow(img,  cmap=\"gray\"); axes[0].set_title(\"Classification: Glioma\"); axes[0].axis(\"off\")\n",
        "axes[1].imshow(mask, cmap=\"gray\"); axes[1].set_title(\"Segmentation Mask\");    axes[1].axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T21:32:43.686941Z",
          "iopub.execute_input": "2025-06-24T21:32:43.687391Z",
          "iopub.status.idle": "2025-06-24T21:32:44.096557Z",
          "shell.execute_reply.started": "2025-06-24T21:32:43.687358Z",
          "shell.execute_reply": "2025-06-24T21:32:44.09533Z"
        },
        "id": "LuZqkCxrtjLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà 7. Histogramas de frecuencias por p√≠xeles\n",
        "\n",
        "P√≠xeles de las im√°genes y las m√°scaras."
      ],
      "metadata": {
        "id": "9D3W4zHutjLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[7] ‚Äî Histograms\n",
        "arr_img  = np.array(img).ravel()\n",
        "arr_mask = np.array(mask).ravel()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(arr_img,  bins=50, alpha=0.7, label=\"Image\")\n",
        "plt.hist(arr_mask, bins=50, alpha=0.7, label=\"Mask\")\n",
        "plt.legend(); plt.title(\"Pixel Intensity Distribution\"); plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T21:33:07.207766Z",
          "iopub.execute_input": "2025-06-24T21:33:07.208174Z",
          "iopub.status.idle": "2025-06-24T21:33:07.57991Z",
          "shell.execute_reply.started": "2025-06-24T21:33:07.208142Z",
          "shell.execute_reply": "2025-06-24T21:33:07.578734Z"
        },
        "id": "dAoym0DHtjLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù 8. Parsing de metadata\n",
        "\n",
        "Componentes del dataset transformados en una tabla. La columna Split dice a cu√°l corte ir√° cada imagen"
      ],
      "metadata": {
        "id": "1zjznQrrtjLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[8] ‚Äî Metadata Table\n",
        "meta = []\n",
        "for sp in splits:\n",
        "    for cls in classes:\n",
        "        # rafael: dependo de este objeto luego, asi que ahora el dataframe tiene TODAS las imagenes\n",
        "        # antes solo tenia un subset\n",
        "        files = glob.glob(os.path.join(BASE_DIR, \"classification_task\", sp, cls, \"*.jpg\"))\n",
        "        for f in files:\n",
        "            parts = os.path.basename(f).split(\"_\")\n",
        "            meta.append({\n",
        "                \"Filename\": os.path.basename(f),\n",
        "                \"Split\": parts[1],\n",
        "                \"Index\": parts[2],\n",
        "                \"Tumor\": parts[3],\n",
        "                \"Plane\": parts[4],\n",
        "                \"Sequence\": parts[5].split(\".\")[0]\n",
        "            })\n",
        "df_meta = pd.DataFrame(meta)\n",
        "print(\"### Sample Filename Metadata\")\n",
        "display(df_meta)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T21:35:51.987869Z",
          "iopub.execute_input": "2025-06-24T21:35:51.988276Z",
          "iopub.status.idle": "2025-06-24T21:35:52.031756Z",
          "shell.execute_reply.started": "2025-06-24T21:35:51.988246Z",
          "shell.execute_reply": "2025-06-24T21:35:52.030807Z"
        },
        "id": "ew0mFzjGtjLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç 9. ejemplo de Overlay de la m√°scara\n",
        "\n",
        "Overlay de la m√°scara en rojo sobre la resonancia."
      ],
      "metadata": {
        "id": "Fm6IiRO3tjLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[9] ‚Äî Overlay\n",
        "img_rgb  = np.array(img.convert(\"RGB\"))\n",
        "mask_arr = np.array(mask)\n",
        "\n",
        "overlay = img_rgb.copy()\n",
        "overlay[mask_arr>0] = [255,0,0]\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(overlay); plt.title(\"Red Overlay = Tumor\"); plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T21:36:45.264016Z",
          "iopub.execute_input": "2025-06-24T21:36:45.264475Z",
          "iopub.status.idle": "2025-06-24T21:36:45.444815Z",
          "shell.execute_reply.started": "2025-06-24T21:36:45.264435Z",
          "shell.execute_reply": "2025-06-24T21:36:45.443781Z"
        },
        "id": "iPLGHM4htjLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî≤ 10. Grid aleatorio de ejemplos de clasificaci√≥n\n",
        "\n",
        "Desplegar un grid 3x3"
      ],
      "metadata": {
        "id": "UND767X6tjLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[10] ‚Äî Random Grid\n",
        "samples = glob.glob(os.path.join(BASE_DIR, \"classification_task/train/*/*.jpg\"))\n",
        "grid = np.random.choice(samples, 9, replace=False)\n",
        "\n",
        "fig, axes = plt.subplots(3,3, figsize=(8,8))\n",
        "axes = axes.flatten()\n",
        "for ax, fp in zip(axes, grid):\n",
        "    im = Image.open(fp)\n",
        "    cls = os.path.basename(fp).split(\"_\")[3]\n",
        "    ax.imshow(im, cmap=\"gray\")\n",
        "    ax.set_title(cls)\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T21:38:29.85519Z",
          "iopub.execute_input": "2025-06-24T21:38:29.855672Z",
          "iopub.status.idle": "2025-06-24T21:38:30.918345Z",
          "shell.execute_reply.started": "2025-06-24T21:38:29.855642Z",
          "shell.execute_reply": "2025-06-24T21:38:30.917494Z"
        },
        "id": "Ps6V87E0tjLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî≤ 11. Dataloader para clasificacion\n"
      ],
      "metadata": {
        "id": "77b9QlV-h2Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "def get_transforms(mean, std):\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "# usar este objeto con cautela. Carga TODAS las imagenes en memoria\n",
        "# para hacer mas rapido el entrenamiento. No deben crearse muchas instancias o\n",
        "# la session crasheara.\n",
        "# opcional dejar 3 canales (para el modelo de huggingface)\n",
        "class BrainTumorDataset(Dataset):\n",
        "    def __init__(self, df, root_dir):\n",
        "        self.label_map = {'gl': 0, 'me': 1, 'no': 2, 'pi': 3}\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.meta = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            tumor_to_file = {\n",
        "                \"gl\":\"glioma\",\n",
        "                \"me\":\"meningioma\",\n",
        "                \"no\":\"no_tumor\",\n",
        "                \"pi\":\"pituitary\"\n",
        "            }\n",
        "            tumor = row['Tumor']\n",
        "            filename = row['Filename']\n",
        "\n",
        "            img_path = os.path.join(root_dir, row['Split'], tumor_to_file[tumor], filename)\n",
        "\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            self.images.append(image)\n",
        "            self.labels.append(self.label_map[tumor])\n",
        "            self.meta.append({\"path\":img_path})\n",
        "\n",
        "        # cargamos las imagenes y calculamos el mean y std para normalizar\n",
        "        mean, std = self._calculate_mean_std()\n",
        "        self.transform = get_transforms(mean, std)\n",
        "\n",
        "    def _calculate_mean_std(self):\n",
        "        as_tensor = transforms.Compose([\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        mean = torch.zeros(3)\n",
        "        std = torch.zeros(3)\n",
        "\n",
        "        for image in self.images:\n",
        "            tensor_img = as_tensor(image)\n",
        "            mean += tensor_img.mean([1, 2])\n",
        "            std += tensor_img.std([1, 2])\n",
        "\n",
        "        mean /= len(self.images)\n",
        "        std /= len(self.images)\n",
        "\n",
        "        return mean.tolist(), std.tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        return {\n",
        "            'pixel_values': self.transform(image).squeeze(0),  # (1, *) -> (*) Remueve la dimensi√≥n del batch\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def get_with_meta(self,idx):\n",
        "        image = self.images[idx]\n",
        "        return image, self.labels[idx], self.meta[idx]\n",
        "\n",
        "def create_dataloaders(batch_size=32):\n",
        "    train_df = df_meta[df_meta['Split'] == 'train']\n",
        "    test_df = df_meta[df_meta['Split'] == 'test']\n",
        "\n",
        "    train_dataset = BrainTumorDataset(train_df, BASE_DIR + 'classification_task')\n",
        "    test_dataset = BrainTumorDataset(test_df, BASE_DIR + 'classification_task')\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader, train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "VzF4YY0eiOgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tarda algo por la carga de las imagenes en memoria. Igual mejor hacerlo ahora que en entrenamiento\n",
        "train_loader, test_loader, train_dataset, test_dataset = create_dataloaders()"
      ],
      "metadata": {
        "id": "EMzy-xSQoYqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# solo correr si se quiere crear nuevas variables ya teniendo otras. por cuestiones de memoria\n",
        "#del train_loader, test_loader, train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "a3c-oSFpir1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT: El Visual Transformer"
      ],
      "metadata": {
        "id": "K15fTyZqyvm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci√≥n, tendr√°n la estructura del Visual Transformer. Su labor ser√° utilizarlo en el task de `clasificaci√≥n` de tumores del dataset `BRICS` haciendo √©nfasis en:\n",
        "\n",
        "1. C√≥mo reajustar un codebase dado para un dataset de ejemplo?\n",
        "1. Entender el modelo ViT para explicarlo a sus compa√±eros.\n",
        "1. C√≥mo utilizar la matriz de atenci√≥n en el caso de ViT? En qu√© ayuda en nuestro ejercicio?\n",
        "1. Comparar nuestro ViT \"casero\" con el ViT de Huggingface.\n",
        "1. Comparar ViT con un modelo de StackedAutoEncoders.\n",
        "1. Utilizar `wandb` o `tensorboard` para el logging de los modelos.\n",
        "1. Proyecto: Crear, en base al dataset dado, uno o unos gr√°ficos custom de `wandb` siguiendo los ejemplos de [Custom charts](https://docs.wandb.ai/guides/app/features/custom-charts/)\n",
        "\n",
        "Tienen libertad en todo el proyecto. Generen un pdf de resumen como hemos hecho en el curso y una presentaci√≥n de no m√°s de 15 diapositivas resumiendo la tarea."
      ],
      "metadata": {
        "id": "cn9tcSYDzWy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seccion de respuesta"
      ],
      "metadata": {
        "id": "iTj5InjWaMYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nuestro ViT"
      ],
      "metadata": {
        "id": "g02aKoxRaS-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h) # Qu√© es esto? Investigar!!!!\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'Importante: La dimensi√≥n de las im√°genes debe ser divisible por el n√∫mero de patches (Dimensi√≥n/Patches = n√∫mero entero)'\n",
        "\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(), # Gaussian error linear unit como funci√≥n de activaci√≥n\n",
        "            nn.Linear(mlp_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p) # Qu√© es esto? Investigar!!!!\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "YSV417CrbS3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())"
      ],
      "metadata": {
        "id": "VN2YXTjpbOtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    # Recuerden que no necesitamos los gradientes en test.\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ],
      "metadata": {
        "id": "8q9M0NjCbQ3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# You can change the architecture here\n",
        "model = ViT(image_size=IMAGE_SIZE, patch_size=IMAGE_SIZE // 2, num_classes=4, channels=3,\n",
        "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
        "model = model.cuda()\n",
        "# We also print the network architecture\n",
        "model\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []"
      ],
      "metadata": {
        "id": "fbhAhTTVcMYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "# Gradually reduce the learning rate while training\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "start_time = time.time()\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('√âpoca:', epoch,'LR:', scheduler.get_last_lr())\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "    scheduler.step()\n",
        "\n",
        "print('Tiempo de ejecuci√≥n:', '{:5.2f}'.format(time.time() - start_time), 'segundos')"
      ],
      "metadata": {
        "id": "1qgXZhU6cxND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huggingface ViT"
      ],
      "metadata": {
        "id": "lqCjSxYHXsKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/google/vit-base-patch16-224"
      ],
      "metadata": {
        "id": "rW2W457wXuTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import ViTImageProcessorFast, ViTForImageClassification, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "pipeline = pipeline(\n",
        "    task=\"image-classification\",\n",
        "    model=\"google/vit-base-patch16-224\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device=0\n",
        ")\n",
        "pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
      ],
      "metadata": {
        "id": "HVNB53WzYCt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = ViTImageProcessorFast.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\") # Por qu√© el nombre es patch16-224?"
      ],
      "metadata": {
        "id": "Y8wIkVlpYHGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    ViTImageProcessorFast,\n",
        "    ViTForImageClassification,\n",
        "    ViTConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ATRjVazk-ggN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_modelo_con_processor(\n",
        "    n_classes=4,\n",
        "    freeze_parameters=True,\n",
        "    pretrained_model=\"google/vit-base-patch16-224\",\n",
        "    debug=True\n",
        "):\n",
        "    \"\"\"\n",
        "    En esta secci√≥n:\n",
        "    1) Llamamos al modelo ViT pre-entrenado\n",
        "    2) Modificamos la cabecera\n",
        "    3) Dedicimos si modificar TODOS los pesos o solamente la capa lineal\n",
        "    \"\"\"\n",
        "    if debug:\n",
        "        print(\"*** Setup modelo con processor ***\")\n",
        "    # Cargamos el processor\n",
        "    processor = ViTImageProcessorFast.from_pretrained(pretrained_model)\n",
        "\n",
        "    # Modificamos la configuraci√≥n del clasificador\n",
        "    config = ViTConfig.from_pretrained(pretrained_model)\n",
        "    config.num_labels = n_classes\n",
        "    config.id2label = {i: str(i) for i in range(n_classes)}\n",
        "    config.label2id = {str(i): i for i in range(n_classes)}\n",
        "\n",
        "    model = ViTForImageClassification.from_pretrained(\n",
        "        pretrained_model,\n",
        "        config=config ,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Modelo cargado con {config.num_labels} clases\")\n",
        "        print(f\"Dimensi√≥n de la capa de clasificaci√≥n (luego del cambio) {model.classifier.weight.shape}\")\n",
        "\n",
        "    # To Freeze or not to Freeze\n",
        "    if freeze_parameters:\n",
        "        for param in model.vit.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    return model, processor, config\n"
      ],
      "metadata": {
        "id": "1DSNcEy1Y4dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute accuracy metrics for evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
        "\n",
        "def fine_tune_con_trainer(model, train_dataset, test_dataset, output_dir=\"./UCV-vit-mnist-finetuned\", debug=True):\n",
        "    \"\"\"Fine-tune using HuggingFace Trainer\"\"\"\n",
        "\n",
        "    # Training arguments\n",
        "    # Verificar https://huggingface.co/docs/transformers/en/main_classes/trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        remove_unused_columns=False,\n",
        "        push_to_hub=False,\n",
        "        report_to=None,  # Disable wandb/tensorboard !!!!!PROYECTO!!!!!\n",
        "    )\n",
        "\n",
        "    # Initialiar el entrenador\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "    if debug:\n",
        "        print(\"Comenzando fine-tuning...\")\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluar el model\n",
        "    if debug:\n",
        "        print(\"Evaluating on test set...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Test Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "\n",
        "    # Salvar el modelo\n",
        "    trainer.save_model()\n",
        "    if debug:\n",
        "        print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "    return trainer, eval_results"
      ],
      "metadata": {
        "id": "OOa0UBleZ4rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference(model, processor, num_samples=5):\n",
        "    \"\"\"Test inference en un split aleatorio de los datos\"\"\"\n",
        "\n",
        "    # Load test dataset\n",
        "    mnist = load_dataset(\"mnist\")\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Get random sample\n",
        "        idx = np.random.randint(0, len(mnist['test']))\n",
        "        sample = mnist['test'][idx]\n",
        "        image = sample['image']\n",
        "        true_label = sample['label']\n",
        "\n",
        "        # Preprocess\n",
        "        rgb_image = image.convert('RGB')\n",
        "        inputs = processor(images=rgb_image, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values'].to(device)\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "            confidence = probabilities.max().item()\n",
        "\n",
        "        # Plot\n",
        "        axes[i].imshow(image, cmap='gray')\n",
        "        axes[i].set_title(f'True: {true_label}\\nPred: {predicted_class}\\nConf: {confidence:.3f}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "M0TKGEFCZ-nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Huggingface ViT fine tuning\")\n",
        "\n",
        "model, processor, config = setup_modelo_con_processor()\n",
        "trainer, eval_results = fine_tune_con_trainer(model, train_dataset, test_dataset)"
      ],
      "metadata": {
        "id": "kJJ2QYhB-VGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention y Transformers\n",
        "\n",
        "En esta secci√≥n, tenemos la implementaci√≥n del mecanismo `self-attention` implementado por el grupo docente de la UCV en `Numpy` y `PyTorch`. Luego, tenemos una implementaci√≥n base de un `ViT` de 6 capas usando el dataset `MNIST`."
      ],
      "metadata": {
        "id": "yLtj22B10CBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificamos que tenemos acceso a la GPU. Si no est√°n conectados, se ver√° como en la primera celda. Si est√°n conectados, como en la segunda."
      ],
      "metadata": {
        "id": "gnLoca1x0WyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "AfQFBhvYyyT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "12WJkFT60Yj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### El mecanismo `Self-attention`\n",
        "\n",
        "Para entender el mecanismo visto en clase, implementamos primero usando `numpy` y de manera \"manual`. Hagan √©nfaseis en las dimensiones y en los c√°lculos en cada secci√≥n cuando multiplicamos las matrices.\n",
        "\n",
        "Modifiquen las variables a conveniencia y vean c√≥mo se modifican las salidas."
      ],
      "metadata": {
        "id": "C4WfKag30pJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "# I. Definimos la entrada X\n",
        "# X consiste de 32 datos, cada uno de ellos de dimensi√≥n 256\n",
        "n = 32\n",
        "d = 256\n",
        "X = randn(n, d) # (32, 256)\n",
        "\n",
        "# II. Generamos los pesos de proyecci√≥n como vimos en clases. Wq, Wk, Wv: queries, keys, values.\n",
        "Wq = randn(d, d) #(256, 256)\n",
        "Wk = randn(d, d)\n",
        "Wv = randn(d, d)\n",
        "\n",
        "# III. Projectamos X para conseguir los vectores de queries, keys y value.\n",
        "Q = np.dot(X, Wq) # (32, 256)\n",
        "K = np.dot(X, Wk)\n",
        "V = np.dot(X, Wv)\n",
        "\n",
        "# IV. Calculamos la matriz de self-attention, almacenada como A.\n",
        "# Luego, aplicamos la funci√≥n softmax\n",
        "# A = softmax(QK^T / \\sqrt{d})\n",
        "\n",
        "# Definimos la funci√≥n SoftMax\n",
        "def softmax(z):\n",
        "    z = np.clip(z, 100, -100) # clip en el caso de tener valores muy grandes\n",
        "    tmp = np.exp(z)\n",
        "    res = np.exp(z) / np.sum(tmp, axis=1)\n",
        "    return res\n",
        "\n",
        "A = softmax(np.dot(Q, K.transpose())/math.sqrt(d)) #(32, 32)\n",
        "\n",
        "# V. Vemos la salida del self-attention\n",
        "# outputs = A * V\n",
        "outputs = np.dot(A, V) #(32, 256)\n",
        "\n",
        "print(\"La salida de atenci√≥n\\n {}\".format(outputs))"
      ],
      "metadata": {
        "id": "_sc2C6c70j00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementando la capa `Self-attention` con PyTorch"
      ],
      "metadata": {
        "id": "KPv0Sb2R1nVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, dim_input, dim_q, dim_v):\n",
        "        '''\n",
        "        dim_input: the dimension of each sample\n",
        "        dim_q: dimension of Q matrix, should be equal to dim_k\n",
        "        dim_v: dimension of V matrix, also the  dimension of the attention output\n",
        "        '''\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_q = dim_q\n",
        "        self.dim_k = dim_q\n",
        "        self.dim_v = dim_v\n",
        "\n",
        "        # Definimos la proyecci√≥n lineal (Igual que en ML)\n",
        "        self.linear_q = nn.Linear(self.dim_input, self.dim_q, bias=False)\n",
        "        self.linear_k = nn.Linear(self.dim_input, self.dim_k, bias=False)\n",
        "        self.linear_v = nn.Linear(self.dim_input, self.dim_v, bias=False)\n",
        "        self._norm_fact = 1 / math.sqrt(self.dim_k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_q = x.shape\n",
        "\n",
        "        q = self.linear_q(x) # (batchsize, seq_len, dim_q)\n",
        "        k = self.linear_k(x) # (batchsize, seq_len, dim_k)\n",
        "        v = self.linear_v(x) # (batchsize, seq_len, dim_v)\n",
        "        print(f'x.shape:{x.shape} \\n Q.shape:{q.shape} \\n K.shape:{k.shape} \\n V.shape:{v.shape}')\n",
        "\n",
        "        dist = torch.bmm(q, k.transpose(1,2)) * self._norm_fact\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "        print('attention matrix: ', dist.shape)\n",
        "\n",
        "        outputs = torch.bmm(dist, v)\n",
        "        print('attention outputs: ', outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "batch_size = 32 # n√∫mero de filas del batch\n",
        "dim_input = 128 # Dimensi√≥n de cada item dentro del batch (cu√°ntas caracter√≠sticas tiene)\n",
        "seq_len = 20 # Dimensi√≥n de la secuencia\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, dim_input)\n",
        "self_attention = SelfAttention(dim_input, dim_q = 64, dim_v = 32)\n",
        "\n",
        "attention = self_attention(x)\n",
        "\n",
        "print(attention)"
      ],
      "metadata": {
        "id": "r-x_qynd1lXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers\n",
        "\n",
        "En esta secci√≥n, implementamos el modelo de 6 capas `ViT` (Vision Transformer) y lo entrenamos en el dataset `MNIST` como ejemplo. Consideramos, en consecuencia, el task de clasificaci√≥n."
      ],
      "metadata": {
        "id": "gzHZ_-Ek185f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### El Dataloader"
      ],
      "metadata": {
        "id": "Y5dWPkXG2MB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def get_mnist_loader(batch_size=100, shuffle=True):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_dataset = MNIST(root='../data',\n",
        "                          train=True,\n",
        "                          transform=torchvision.transforms.ToTensor(),\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='../data',\n",
        "                         train=False,\n",
        "                         transform=torchvision.transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=shuffle)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "IUnG14D8147u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### El modelo ViT\n",
        "\n",
        "Recordemos que cada bloque de Transformer tiene 2 m√≥dulos:\n",
        "\n",
        "1. El de Self-attention\n",
        "1. El Lineal"
      ],
      "metadata": {
        "id": "OBp7NT8e2O4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h) # Qu√© es esto? Investigar!!!!\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'Importante: La dimensi√≥n de las im√°genes debe ser divisible por el n√∫mero de patches (Dimensi√≥n/Patches = n√∫mero entero)'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(), # Gaussian error linear unit como funci√≥n de activaci√≥n\n",
        "            nn.Linear(mlp_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p) # Qu√© es esto? Investigar!!!!\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "yh8Z2ljk2O9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento y test"
      ],
      "metadata": {
        "id": "RpK1Qmuh25w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())"
      ],
      "metadata": {
        "id": "ZR-Q7ipN25Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    # Recuerden que no necesitamos los gradientes en test.\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ],
      "metadata": {
        "id": "HStMzH-v287J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenando!!"
      ],
      "metadata": {
        "id": "H7Yi7H2r3CNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# You can change the architecture here\n",
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
        "model = model.cuda()\n",
        "# We also print the network architecture\n",
        "model\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []"
      ],
      "metadata": {
        "id": "-Ka8OBTM3BSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "# Gradually reduce the learning rate while training\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('√âpoca:', epoch,'LR:', scheduler.get_last_lr())\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "    scheduler.step()\n",
        "\n",
        "print('Tiempo de ejecuci√≥n:', '{:5.2f}'.format(time.time() - start_time), 'segundos')"
      ],
      "metadata": {
        "id": "TU-SxGer3FR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugginface vs UCV\n",
        "\n",
        "Ahora, comparemos el rendimiento de nuestro `ViT` con el de Hugginface usando el tutorial de Hugginface [Vision Transformer (ViT)](https://huggingface.co/docs/transformers/en/model_doc/vit)\n",
        "\n",
        "El tutorial b√°sico para usar un pipeline es el siguiente:"
      ],
      "metadata": {
        "id": "gvhm8WLZ4lHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import ViTImageProcessorFast, ViTForImageClassification, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "pipeline = pipeline(\n",
        "    task=\"image-classification\",\n",
        "    model=\"google/vit-base-patch16-224\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "gEZrw95s3Lhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
      ],
      "metadata": {
        "id": "29Ase3wQ6VqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El problema fundamental para nosotros, es que el modelo `google/vit-base-patch16-224` acepta im√°genes $224 \\times 224$.\n",
        "\n",
        "Por lo tanto, tenemos que usar el `ViTImageProcessorFast` para poder escalarlas.\n",
        "\n",
        "Hacemos el proceso para MNIST"
      ],
      "metadata": {
        "id": "dI5DNDI95uS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = ViTImageProcessorFast.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\") # Por qu√© el nombre es patch16-224?"
      ],
      "metadata": {
        "id": "HUm9Zicc55io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = load_dataset(\"mnist\")"
      ],
      "metadata": {
        "id": "StT-h-EG6tvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "primera_imagen = mnist[\"train\"][0][\"image\"]"
      ],
      "metadata": {
        "id": "N9QE6-YE6x1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "primera_imagen"
      ],
      "metadata": {
        "id": "frKR9cTD62nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(primera_imagen.size)"
      ],
      "metadata": {
        "id": "GOqq7nKF65sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(primera_imagen.mode)"
      ],
      "metadata": {
        "id": "b5mqpafg69kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if primera_imagen.mode != \"RGB\":\n",
        "    primera_imagen = primera_imagen.convert('RGB')"
      ],
      "metadata": {
        "id": "swDl9dy06_md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "primera_imagen"
      ],
      "metadata": {
        "id": "mL-rrmQC7LUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "primera_imagen.mode"
      ],
      "metadata": {
        "id": "bxmA_fxs7OZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(primera_imagen, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "sLAzDSyR7Q5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "id": "qVqLjBWv7YWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape de la imagen\", inputs[\"pixel_values\"].shape)"
      ],
      "metadata": {
        "id": "57Nv-7k17ZE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F√≠jense c√≥mo ahora la imagen es `(1, 3, 224, 224)` referente a:\n",
        "\n",
        "1. Batch size: 1\n",
        "1. Canales: 3\n",
        "1. Ancho: 224\n",
        "1. Alto: 224"
      ],
      "metadata": {
        "id": "XpnipXY37nN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predicciones = softmax(outputs.logits, dim=-1)\n",
        "    clase_id = predicciones.argmax().item()\n",
        "    confianza = predicciones.max().item()\n",
        "\n",
        "print(f\"La clase es {clase_id}\")\n",
        "print(f\"La confianza es {confianza}\")\n"
      ],
      "metadata": {
        "id": "oz3IsujE7cR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como el `ViT` de huggingface fue entrenado para 1000 clases. Debemos reutilizarlo y modificarlo para las clases de MNIST.\n",
        "\n",
        "Recordemos que el proceso de reutilizar un modelo para modificar solamente las capas de clasificaci√≥n se conoce como `fine-tuning`"
      ],
      "metadata": {
        "id": "s3PO2y-69OoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning con MNIST"
      ],
      "metadata": {
        "id": "Plo196u_9syP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    ViTImageProcessorFast,\n",
        "    ViTForImageClassification,\n",
        "    ViTConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "sc--qVb272rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creando el Dataset"
      ],
      "metadata": {
        "id": "Hq7aCo669yrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataset(Dataset):\n",
        "    \"\"\"Custom Dataset para MNIST con ViT preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, processor, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = item['image']\n",
        "        label = item['label']\n",
        "\n",
        "        # Convert grayscale to RGB\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Apply custom transforms if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Process with ViT processor\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            'pixel_values': inputs['pixel_values'].squeeze(0),  # (1, *) -> (*) Remueve la dimensi√≥n del batch\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "XBhreOyd9wTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup y procesador\n",
        "\n",
        "En esta secci√≥n, cargamos el modelo pre entrenado de Huggingface y lo modificamos al task de clasificaci√≥n de MNIST.\n",
        "\n",
        "F√≠jense que la cantidad de clases a usar depende de nosotros (cambiando el par√°metro)"
      ],
      "metadata": {
        "id": "5VoJSxTvBhl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_modelo_con_processor(\n",
        "    n_classes=10, # Default 10 porque es MNIST\n",
        "    freeze_parameters=True,\n",
        "    pretrained_model=\"google/vit-base-patch16-224\",\n",
        "    debug=True\n",
        "):\n",
        "    \"\"\"\n",
        "    En esta secci√≥n:\n",
        "    1) Llamamos al modelo ViT pre-entrenado\n",
        "    2) Modificamos la cabecera\n",
        "    3) Dedicimos si modificar TODOS los pesos o solamente la capa lineal\n",
        "    \"\"\"\n",
        "    if debug:\n",
        "        print(\"*** Setup modelo con processor ***\")\n",
        "    # Cargamos el processor\n",
        "    processor = ViTImageProcessorFast.from_pretrained(pretrained_model)\n",
        "\n",
        "    # Modificamos la configuraci√≥n del clasificador\n",
        "    config = ViTConfig.from_pretrained(pretrained_model)\n",
        "    config.num_labels = n_classes\n",
        "    config.id2label = {i: str(i) for i in range(n_classes)}\n",
        "    config.label2id = {str(i): i for i in range(n_classes)}\n",
        "\n",
        "    model = ViTForImageClassification.from_pretrained(\n",
        "        pretrained_model,\n",
        "        config=config ,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Modelo cargado con {config.num_labels} clases\")\n",
        "        print(f\"Dimensi√≥n de la capa de clasificaci√≥n (luego del cambio) {model.classifier.weight.shape}\")\n",
        "\n",
        "    # To Freeze or not to Freeze\n",
        "    if freeze_parameters:\n",
        "        for param in model.vit.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    return model, processor, config\n"
      ],
      "metadata": {
        "id": "4nj2SQGY9_UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga y preparaci√≥n de los datos\n",
        "\n",
        "Esta secci√≥n usa el MNIST dataset creado antes para devolver el train_dataset y el test_dataset"
      ],
      "metadata": {
        "id": "vJcqEDhZBuv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cargar_y_preparar_datos(processor, debug=True):\n",
        "    \"\"\"Cargar datos usando el processor\"\"\"\n",
        "    if debug:\n",
        "        print(\"*** Cargar y preparar datos ***\")\n",
        "    # Ejemplo con MNIST\n",
        "    # Ac√° se puede modificar para tumores\n",
        "    mnist = load_dataset(\"mnist\")\n",
        "\n",
        "    # Crear datasets\n",
        "    train_dataset = MNISTDataset(mnist['train'], processor)\n",
        "    test_dataset = MNISTDataset(mnist['test'], processor)\n",
        "    if debug:\n",
        "        print(f\"Datos de entrenamiento: {len(train_dataset)}\")\n",
        "        print(f\"Datos de test: {len(test_dataset)}\")\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "eFuW2P3kAvEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer"
      ],
      "metadata": {
        "id": "s9r_1yu8B4N9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute accuracy metrics for evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
        "\n",
        "def fine_tune_con_trainer(model, train_dataset, test_dataset, output_dir=\"./UCV-vit-mnist-finetuned\", debug=True):\n",
        "    \"\"\"Fine-tune using HuggingFace Trainer\"\"\"\n",
        "\n",
        "    # Training arguments\n",
        "    # Verificar https://huggingface.co/docs/transformers/en/main_classes/trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        remove_unused_columns=False,\n",
        "        push_to_hub=False,\n",
        "        report_to=None,  # Disable wandb/tensorboard !!!!!PROYECTO!!!!!\n",
        "    )\n",
        "\n",
        "    # Initialiar el entrenador\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "    if debug:\n",
        "        print(\"Comenzando fine-tuning...\")\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluar el model\n",
        "    if debug:\n",
        "        print(\"Evaluating on test set...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Test Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "\n",
        "    # Salvar el modelo\n",
        "    trainer.save_model()\n",
        "    if debug:\n",
        "        print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "    return trainer, eval_results"
      ],
      "metadata": {
        "id": "9Bc6HYcZB5uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m2LUrKEAhet6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference(model, processor, num_samples=5):\n",
        "    \"\"\"Test inference en un split aleatorio de los datos\"\"\"\n",
        "\n",
        "    # Load test dataset\n",
        "    mnist = load_dataset(\"mnist\")\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Get random sample\n",
        "        idx = np.random.randint(0, len(mnist['test']))\n",
        "        sample = mnist['test'][idx]\n",
        "        image = sample['image']\n",
        "        true_label = sample['label']\n",
        "\n",
        "        # Preprocess\n",
        "        rgb_image = image.convert('RGB')\n",
        "        inputs = processor(images=rgb_image, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values'].to(device)\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "            confidence = probabilities.max().item()\n",
        "\n",
        "        # Plot\n",
        "        axes[i].imshow(image, cmap='gray')\n",
        "        axes[i].set_title(f'True: {true_label}\\nPred: {predicted_class}\\nConf: {confidence:.3f}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7S28KHTuhezg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salvando y carga en el caso de modelos salvados"
      ],
      "metadata": {
        "id": "4O1jZVoehp-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_for_deployment(model, processor, save_path=\"./vit-mnist-final\", load=False, debug=True):\n",
        "    \"\"\"\n",
        "    Salvar y upload de modelos\n",
        "    \"\"\"\n",
        "    if load:\n",
        "            # Test loading\n",
        "        try:\n",
        "            loaded_model = ViTForImageClassification.from_pretrained(save_path)\n",
        "            loaded_processor = ViTImageProcessorFast.from_pretrained(save_path)\n",
        "            if debug:\n",
        "                print(\"‚úì Modelo y processor cargados correctamente!\")\n",
        "            return loaded_model, loaded_processor\n",
        "        except Exception as e:\n",
        "            print(f\"‚úó Error loading model: {e}\")\n",
        "            return None, None\n",
        "    else:\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        # Save model\n",
        "        model.save_pretrained(save_path)\n",
        "\n",
        "        # Save processor\n",
        "        processor.save_pretrained(save_path)\n",
        "\n",
        "        if debug:\n",
        "            print(f\"Modelo y processor salvados en {save_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SiZYRll9hqF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning pipeline\n",
        "\n",
        "1. Setup y processor\n",
        "1. Cargar datos\n",
        "1. Entrenar"
      ],
      "metadata": {
        "id": "e-hre4Qr_9HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ViT fine tuning\")\n",
        "\n",
        "model, processor, config = setup_modelo_con_processor()\n",
        "train_dataset, test_dataset = cargar_y_preparar_datos(processor)\n",
        "trainer, eval_results = fine_tune_con_trainer(model, train_dataset, test_dataset)"
      ],
      "metadata": {
        "id": "mgD12UIZ_5KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo de predicciones"
      ],
      "metadata": {
        "id": "_4_FiG3Jh8zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_inference(model, processor, num_samples=5)"
      ],
      "metadata": {
        "id": "9uT3doyCAU-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salvando modelo en directorio custom"
      ],
      "metadata": {
        "id": "hy6LfrKViaOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colocamos la bandera `load=False` si solo queremos salvar"
      ],
      "metadata": {
        "id": "Z6ifZmwMijkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_model_for_deployment(model, processor, save_path=\"./vit-mnist-final\", load=False)"
      ],
      "metadata": {
        "id": "FIcD6g3xh7oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargando un modelo salvado"
      ],
      "metadata": {
        "id": "vSCz6K-risAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_saved, processor_saved = save_model_for_deployment(model=None, processor=None, save_path=\"./vit-mnist-final\", load=True)"
      ],
      "metadata": {
        "id": "Q5s9EMIuioAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inference(model_saved, processor_saved, num_samples=5)"
      ],
      "metadata": {
        "id": "8BS4-XcCi300"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKHRJ8tMXU9S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}